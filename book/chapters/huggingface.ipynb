{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook assumes you have ran the huggingface-cli login command in the terminal/notebook, and have write permissions to the HuggingFace datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from util import ImageDataset, readSetFromFile\n",
    "\n",
    "def load_images(\n",
    "    masks_glob, \n",
    "    includes_file=None,\n",
    "    imagery_folder=\"imagery/\",\n",
    "    masks_folder=\"masks/\", \n",
    "    fraction=0.1, \n",
    "    exclude=[]\n",
    "):\n",
    "    include_masks = list(readSetFromFile(includes_file, str)) if includes_file else None\n",
    "\n",
    "    train_data = ImageDataset(\n",
    "        \"../learning/\" + imagery_folder, \n",
    "        \"../learning/\" + masks_folder, \n",
    "        masks_glob, \n",
    "        include_masks=include_masks, \n",
    "        exclude_masks=exclude, \n",
    "        subset=\"Train\", \n",
    "        fraction=fraction,\n",
    "    )\n",
    "    print(\"\")\n",
    "    val_data = ImageDataset(\n",
    "        \"../learning/\" + imagery_folder, \n",
    "        \"../learning/\" + masks_folder, \n",
    "        masks_glob, \n",
    "        include_masks=include_masks, \n",
    "        exclude_masks=exclude, \n",
    "        subset=\"Test\", \n",
    "        fraction=fraction,\n",
    "    )\n",
    "\n",
    "    train_image_names = [str(path) for path in train_data.image_names]\n",
    "    train_mask_names = [str(path) for path in train_data.mask_names]\n",
    "\n",
    "    val_image_names = [str(path) for path in val_data.image_names]\n",
    "    val_mask_names = [str(path) for path in val_data.mask_names]\n",
    "\n",
    "    return (train_data.names, train_image_names, train_mask_names), (val_data.names, val_image_names, val_mask_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict, Image\n",
    "\n",
    "def create_dataset(names, image_paths, label_paths):\n",
    "    dataset = Dataset.from_dict({\"image\": image_paths, \"label\": label_paths, \"name\": names})\n",
    "    dataset = dataset.cast_column(\"image\", Image())\n",
    "    dataset = dataset.cast_column(\"label\", Image())\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and loaded 4382 images with glob *_corrected.png.\n",
      "Pruned 4156 masks based on set of 0 included masks.\n",
      "Pruned 0 masks from set of 0 excluded masks.\n",
      "Subset of 226 ground truth segmentation masks marked for Train.\n",
      "\n",
      "Found and loaded 4382 images with glob *_corrected.png.\n",
      "Pruned 4156 masks based on set of 0 included masks.\n",
      "Pruned 0 masks from set of 0 excluded masks.\n",
      "Subset of 0 ground truth segmentation masks marked for Test.\n"
     ]
    }
   ],
   "source": [
    "paths_manual, _ = load_images(\"*_corrected.png\", \"../data/train_images.txt\", fraction=0)\n",
    "validation_dataset_manual = create_dataset(*paths_manual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The extra image names set to be excluded are additional datapoints used for validation experiments and must not be trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from util import writeSetToFile\n",
    "\n",
    "exclude_names = paths_manual[0]\n",
    "writeSetToFile(\"../data/exclude_images.txt\", exclude_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[stodoran/elwha-segmentation-v1](https://huggingface.co/datasets/stodoran/elwha-segmentation-v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and loaded 4382 images with glob *_binary.png.\n",
      "Pruned 3109 masks based on set of 226 included masks.\n",
      "Pruned 162 masks from set of 226 excluded masks.\n",
      "Subset of 1111 ground truth segmentation masks marked for Train.\n",
      "\n",
      "Found and loaded 4382 images with glob *_binary.png.\n",
      "Pruned 3109 masks based on set of 226 included masks.\n",
      "Pruned 162 masks from set of 226 excluded masks.\n",
      "Subset of 0 ground truth segmentation masks marked for Test.\n"
     ]
    }
   ],
   "source": [
    "train_paths_v1, _ = load_images(\"*_binary.png\", \"../data/useful_images.txt\", fraction=0, exclude=exclude_names)\n",
    "train_dataset_v1 = create_dataset(*train_paths_v1)\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train\": train_dataset_v1,\n",
    "    \"validation\": validation_dataset_manual,\n",
    "})\n",
    "# dataset.push_to_hub(\"stodoran/elwha-segmentation-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[stodoran/elwha-segmentation-v2](https://huggingface.co/datasets/stodoran/elwha-segmentation-v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and loaded 1148 images with glob *[!_manualfix].png.\n",
      "Pruned 91 masks from set of 226 excluded masks.\n",
      "Subset of 1057 ground truth segmentation masks marked for Train.\n",
      "\n",
      "Found and loaded 1148 images with glob *[!_manualfix].png.\n",
      "Pruned 91 masks from set of 226 excluded masks.\n",
      "Subset of 0 ground truth segmentation masks marked for Test.\n"
     ]
    }
   ],
   "source": [
    "train_paths_v2, _ = load_images(\"*[!_manualfix].png\", masks_folder=\"corrections_v1/\", fraction=0, exclude=exclude_names)\n",
    "train_dataset_v2 = create_dataset(*train_paths_v2)\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train\": train_dataset_v2,\n",
    "    \"validation\": validation_dataset_manual,\n",
    "})\n",
    "# dataset.push_to_hub(\"stodoran/elwha-segmentation-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[stodoran/elwha-segmentation-predict](https://huggingface.co/datasets/stodoran/elwha-segmentation-predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and loaded 4382 images with glob *.png.\n",
      "Pruned 0 masks from set of 0 excluded masks.\n",
      "Subset of 4382 ground truth segmentation masks marked for Train.\n",
      "\n",
      "Found and loaded 4382 images with glob *.png.\n",
      "Pruned 0 masks from set of 0 excluded masks.\n",
      "Subset of 0 ground truth segmentation masks marked for Test.\n"
     ]
    }
   ],
   "source": [
    "train_paths_pred, _ = load_images(\"*.png\", imagery_folder=\"imagery/\", masks_folder=\"imagery/\", fraction=0)\n",
    "train_dataset_pred = create_dataset(*train_paths_pred)\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"data\": train_dataset_pred,\n",
    "})\n",
    "# dataset.push_to_hub(\"stodoran/elwha-segmentation-predict\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[stodoran/elwha-segmentation-all](https://huggingface.co/datasets/stodoran/elwha-segmentation-all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and loaded 4382 images with glob *_binary.png.\n",
      "Pruned 226 masks from set of 226 excluded masks.\n",
      "Subset of 4156 ground truth segmentation masks marked for Train.\n",
      "\n",
      "Found and loaded 4382 images with glob *_binary.png.\n",
      "Pruned 226 masks from set of 226 excluded masks.\n",
      "Subset of 0 ground truth segmentation masks marked for Test.\n"
     ]
    }
   ],
   "source": [
    "train_paths_all, _ = load_images(\"*_binary.png\", fraction=0, exclude=exclude_names)\n",
    "train_dataset_all = create_dataset(*train_paths_all)\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train\": train_dataset_all,\n",
    "    \"validation\": validation_dataset_manual,\n",
    "})\n",
    "# dataset.push_to_hub(\"stodoran/elwha-segmentation-all\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "elwha_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
